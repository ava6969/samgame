runner:
  run: PPO
  checkpoint_freq: 20

  stop:
    timesteps_total: 1300000000
    training_iteration: 130000000
  config:
    env: SAMTrader-v0
    framework: torch
    num_workers: 8
    num_gpus: 1
#    num_envs_per_worker: 8
#    sgd_minibatch_size: 120
#    vf_share_layers: True
#    num_sgd_iter: 32

#    rollout_fragment_length: 60 # trajectory = 1 hour of trade
#    train_batch_size: 3840 # full day 60 * 8 * 8
    vf_clip_param: 100000
    vf_loss_coeff: 0.000001

    model:
      custom_model: mlstm_net
#      custom_model_config:
#        train: True
#      use_lstm: True
      fcnet_hiddens: [100, 128]
      fcnet_activation: relu
      lstm_cell_size: 128
#      lstm_use_prev_action: True

    # Calculate rewards but don't reset the environment when the horizon is
    # hit. This allows value estimation and RNN state to span across logical
    # episodes denoted by horizon. This only has an effect if horizon != inf.


    env_config:
      use_image: False
      initial_cash: 25000
      tech_indicators: MA EMA ATR ROC
      log_every: 30000
      n_symbols: 4
      reward_type: PC
      max_shares: 3
      bins: 7
      window: 200
      episode_range: [500, 2000]
      test: False
      start_date: "2004-01-01 00:00:00"
      end_date: "2019-12-01 00:00:00"